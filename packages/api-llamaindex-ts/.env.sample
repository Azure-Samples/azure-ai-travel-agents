# LLM Provider Selection
# Choose one of: azure-openai, github-models, docker-models, ollama-models, foundry-local
LLM_PROVIDER=azure-openai

# Azure OpenAI Configuration (for LLM_PROVIDER=azure-openai)
AZURE_OPENAI_ENDPOINT=https://PROJECT-NAME.openai.azure.com/openai/v1/
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_DEPLOYMENT=gpt-5
AZURE_OPENAI_API_VERSION=2024-02-15-preview
# Optional: Azure Client ID for Managed Identity
# AZURE_CLIENT_ID=
# Set to true when running in local Docker environment
# IS_LOCAL_DOCKER_ENV=false

# GitHub Models Configuration (for LLM_PROVIDER=github-models)
# GITHUB_TOKEN=
# GITHUB_MODEL=openai/gpt-5

# Docker Models Configuration (for LLM_PROVIDER=docker-models)
# DOCKER_MODEL_ENDPOINT=http://localhost:12434/engines/llama.cpp/v1
# DOCKER_MODEL=ai/phi4:14B-Q4_0

# Ollama Models Configuration (for LLM_PROVIDER=ollama-models)
# OLLAMA_MODEL_ENDPOINT=http://localhost:11434/v1
# OLLAMA_MODEL=llama3.1

# Foundry Local Configuration (for LLM_PROVIDER=foundry-local)
# AZURE_FOUNDRY_LOCAL_MODEL_ALIAS=phi-3.5-mini

# MCP Server URLs
# Update these URLs to match your local or production environment
MCP_CUSTOMER_QUERY_URL=http://mcp-customer-query:5001
MCP_DESTINATION_RECOMMENDATION_URL=http://mcp-destination-recommendation:5002
MCP_ITINERARY_PLANNING_URL=http://mcp-itinerary-planning:5003
MCP_ECHO_PING_URL=http://mcp-echo-ping:5004
MCP_ECHO_PING_ACCESS_TOKEN=123-this-is-a-fake-token-please-use-a-token-provider

# Server Configuration
PORT=4001
LOG_LEVEL=INFO

# OpenTelemetry Configuration
OTEL_SERVICE_NAME=api-llamaindex-ts
OTEL_EXPORTER_OTLP_ENDPOINT=http://aspire-dashboard:18889
OTEL_EXPORTER_OTLP_HEADERS=x-otlp-header=header-value
