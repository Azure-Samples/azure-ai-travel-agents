# Select the LLM provider
# LLM_PROVIDER=azure-openai
# LLM_PROVIDER=github-models
LLM_PROVIDER=local-foundry

# Use Azure Local Foundry
# You can find a list of available models by running the
# following command in your terminal: `foundry model list`.
# Note: make sure to set the LLM_PROVIDER to local-foundry
AZURE_FOUNDRY_LOCAL_MODEL_ALIAS=phi-4-mini-reasoning

# Azure OpenAI settings
# Note: make sure to set the LLM_PROVIDER to azure-openai
AZURE_OPENAI_ENDPOINT=https://PROJECT-NAME.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT_NAME=gpt-4o
# Uncomment the following line to use a specific Azure OpenAI API key
# AZURE_OPENAI_API_KEY=

# Github settings
# Uncomment the following lines to use GitHub models
# Note: make sure to set the LLM_PROVIDER to github-models
# GITHUB_TOKEN=
# GITHUB_MODEL=openai/gpt-4o

# Tools settings (check hostnames in docker-compose)
# Update these URLs to match your local or production environment if needed
MCP_CUSTOMER_QUERY_URL=http://tool-customer-query:8080
MCP_DESTINATION_RECOMMENDATION_URL=http://tool-destination-recommendation:5002
MCP_ITINERARY_PLANNING_URL=http://tool-itinerary-planning:5003
MCP_CODE_EVALUATION_URL=http://tool-code-evaluation:5004
MCP_MODEL_INFERENCE_URL=http://tool-model-inference:5005
MCP_WEB_SEARCH_URL=http://tool-web-search:5000
MCP_ECHO_PING_URL=http://tool-echo-ping:3000
MCP_ECHO_PING_ACCESS_TOKEN=123-this-is-a-fake-token-please-use-a-token-provider

# Opentelemetry settings
OTEL_SERVICE_NAME=api
OTEL_EXPORTER_OTLP_ENDPOINT=http://aspire-dashboard:18889
OTEL_EXPORTER_OTLP_HEADERS=x-otlp-header=header-value