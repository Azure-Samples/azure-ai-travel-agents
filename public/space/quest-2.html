<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Stellar LLM Provider Strategy - Repo Adventure</title>
    <link rel="stylesheet" href="assets/theme.css">
    <link rel="stylesheet" href="../assets/shared/quest-navigator.css">
    <link rel="stylesheet" href="../assets/theme-toggle.css">
    <!-- Prism.js for syntax highlighting -->
    <!-- Using minimal theme since we override all styles with our custom theme -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script>
        // Configure Prism autoloader for syntax highlighting
        if (window.Prism && window.Prism.plugins && window.Prism.plugins.autoloader) {
            window.Prism.plugins.autoloader.languages_path = 'https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/';
        }

        // Trigger Prism highlighting after page load
        document.addEventListener('DOMContentLoaded', function() {
            if (window.Prism) {
                window.Prism.highlightAll();
            }
        });
    </script>
</head>
<body class="theme-space">

    <nav class="navbar">
        <div class="nav-content">
            <div class="nav-left">
                <a href="https://github.com/Azure-Samples/azure-ai-travel-agents" target="_blank" rel="noopener noreferrer" class="github-link">
                    <img src="../assets/shared/github-mark-white.svg" alt="GitHub" width="24" height="24">
                </a>
                <a href="index.html">Galactic Codebase Odyssey: The Azure AI Travel Agents</a>
            </div>
            <div class="nav-middle">
            </div>
            <div class="nav-right">
                <a href="../index.html" class="nav-link">Change Theme</a>
                <a href="#" class="nav-link quest-map-trigger">Quests</a>
                <button class="theme-toggle-btn" aria-label="Switch to light mode" type="button">
                    <div class="theme-toggle-slider">
                        <svg class="toggle-icon sun-icon" viewBox="0 0 24 24">
                            <circle cx="12" cy="12" r="4" fill="currentColor"/>
                            <path d="M12 2v2M12 20v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M2 12h2M20 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                        </svg>
                        <svg class="toggle-icon moon-icon" viewBox="0 0 24 24">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                        </svg>
                    </div>
                </button>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="quest-content">
    <h1>Quest 2: Flexible LLM Provider Strategy</h1>
<hr>
<p>As the <strong>Starship LlamaIndex</strong> glides through the cosmic expanse, the crew faces a critical challenge: adapting to diverse stellar LLM providers. The galaxy is dotted with Azure OpenAI endpoints, GitHub-hosted models, and Ollama‚Äôs local inference systems. To ensure seamless interstellar operations, the ship&#39;s orchestration engine must dynamically select and configure the most suitable LLM provider based on the mission parameters. The crew gathers in the Data Core, where the provider selection module hums with energy, ready for exploration and refinement.</p>
<h2>Key Takeaways</h2>
<p>After completing this quest, you will understand:</p>
<ul>
<li>üéØ <strong>Dynamic LLM Selection</strong>: How to configure and switch between multiple LLM providers using environment variables.</li>
<li>üîç <strong>Azure Authentication</strong>: How to handle both local and managed identity authentication for Azure OpenAI.</li>
<li>‚ö° <strong>Error Handling</strong>: How to safeguard provider selection with robust error messaging for invalid configurations.</li>
<li>üí° <strong>Extensibility</strong>: How to add new LLM providers with minimal changes to the existing architecture.</li>
</ul>
<h2>File Exploration</h2>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/index.ts</code></a></h3>
<p>This file serves as the central orchestrator for LLM provider configuration. It dynamically selects the appropriate provider based on the <code class="inline-code">LLM_PROVIDER</code> environment variable. The modular design ensures that adding new providers is straightforward, while robust error handling guarantees system stability in case of misconfigurations.</p>
<h4>Highlights</h4>
<ul>
<li><code class="inline-code">LLMProvider</code> defines the valid provider types, ensuring type safety and clarity.</li>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> dynamically selects and initializes the appropriate LLM provider based on the environment configuration.</li>
<li>Error handling ensures that invalid or missing provider configurations are flagged immediately, preventing runtime issues.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">type LLMProvider =
  | &quot;azure-openai&quot;
  | &quot;github-models&quot;
  | &quot;foundry-local&quot;
  | &quot;docker-models&quot;
  | &quot;ollama-models&quot;;

const provider = (process.env.LLM_PROVIDER || &quot;&quot;) as LLMProvider;

export const llm = async () =&gt; {
  switch (provider) {
    case &quot;azure-openai&quot;:
      return azureOpenAI();
    case &quot;github-models&quot;:
      return githubModels();
    case &quot;docker-models&quot;:
      return dockerModels();
    case &quot;ollama-models&quot;:
      return ollamaModels();
    case &quot;foundry-local&quot;:
      return foundryLocal();
    default:
      throw new Error(
        `Unknown LLM_PROVIDER &quot;${provider}&quot;. Valid options are: azure-openai, github-models, foundry-local, docker-models, ollama-models.`
      );
  }
};
</code></pre>
<ul>
<li>The <code class="inline-code">LLMProvider</code> type ensures strict adherence to predefined provider options, reducing errors during development.</li>
<li>The <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function uses a switch-case structure for clear and maintainable provider selection logic.</li>
<li>The default case in the switch statement provides robust error handling, ensuring misconfigurations are caught early.</li>
<li>Environment variable <code class="inline-code">LLM_PROVIDER</code> allows dynamic switching of providers without code changes.</li>
<li>This design makes it easy to extend the system by simply adding new cases for additional providers.</li>
</ul>
<hr>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/azure-openai.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/azure-openai.ts</code></a></h3>
<p>This file configures the Azure OpenAI provider, handling authentication for both local and production environments. It supports API key authentication for local Docker setups and Managed Identity Credential for Azure-hosted deployments.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> dynamically configures Azure OpenAI with environment-specific authentication.</li>
<li><code class="inline-code">DefaultAzureCredential</code> and <code class="inline-code">ManagedIdentityCredential</code> handle secure authentication in production environments.</li>
<li>Local Docker setups fall back to API key-based authentication, ensuring flexibility during development.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">const AZURE_COGNITIVE_SERVICES_SCOPE =
  &quot;https://cognitiveservices.azure.com/.default&quot;;

export const llm = async () =&gt; {
  console.log(&quot;Using Azure OpenAI&quot;);

  const isRunningInLocalDocker = process.env.IS_LOCAL_DOCKER_ENV === &quot;true&quot;;
  
  if (isRunningInLocalDocker) {
    console.log(
      &quot;Running in local Docker environment, Azure Managed Identity is not supported. Authenticating with apiKey.&quot;
    );
    
    return openai({
      azure: {
        endpoint: process.env.AZURE_OPENAI_ENDPOINT,
        deployment: process.env.AZURE_OPENAI_DEPLOYMENT,
        apiKey: process.env.AZURE_OPENAI_API_KEY,
      },
    });
  }
  
  let credential: any = new DefaultAzureCredential();
  const clientId = process.env.AZURE_CLIENT_ID;
  if (clientId) {
    console.log(&quot;Using Azure Client ID:&quot;, clientId);
    credential = new ManagedIdentityCredential({
      clientId,
    });
  }

  const azureADTokenProvider = getBearerTokenProvider(
    credential,
    AZURE_COGNITIVE_SERVICES_SCOPE
  );

  return openai({
    azure: {
      azureADTokenProvider,
      endpoint: process.env.AZURE_OPENAI_ENDPOINT,
      deployment: process.env.AZURE_OPENAI_DEPLOYMENT,
    },
  });
};
</code></pre>
<ul>
<li>The <code class="inline-code">isRunningInLocalDocker</code> flag ensures the correct authentication method is used based on the environment.</li>
<li><code class="inline-code">DefaultAzureCredential</code> simplifies authentication in Azure-hosted environments, supporting multiple identity types.</li>
<li>API key authentication provides a fallback for local development, ensuring compatibility across environments.</li>
<li>The <code class="inline-code">getBearerTokenProvider()</code> function integrates seamlessly with Azure AD, enabling secure token-based authentication.</li>
<li>This approach balances security and development flexibility, making the system robust and developer-friendly.</li>
</ul>
<hr>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/github-models.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/github-models.ts</code></a></h3>
<p>This file configures the GitHub-hosted LLM provider. It demonstrates how to integrate third-party services with minimal configuration.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> initializes the GitHub model provider with a base URL and API key.</li>
<li>The <code class="inline-code">GITHUB_TOKEN</code> environment variable ensures secure authentication.</li>
<li>The modular design allows easy integration of additional third-party providers.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">export const llm = async () =&gt; {
  console.log(&quot;Using GitHub Models&quot;);
  return openai({
    baseURL: &quot;https://models.inference.ai.azure.com&quot;,
    apiKey: process.env.GITHUB_TOKEN,
    model: process.env.GITHUB_MODEL,
  });
};
</code></pre>
<ul>
<li>The <code class="inline-code">baseURL</code> parameter specifies the endpoint for GitHub-hosted models, ensuring proper routing of requests.</li>
<li>Authentication is handled securely via the <code class="inline-code">GITHUB_TOKEN</code> environment variable.</li>
<li>The <code class="inline-code">model</code> parameter allows dynamic selection of the specific GitHub-hosted model to use.</li>
<li>This modular design simplifies the process of adding new providers, as each provider is encapsulated in its own file.</li>
<li>The clear separation of concerns ensures that updates to one provider do not impact others.</li>
</ul>
<hr>
<h2>Helpful Hints</h2>
<ul>
<li>Use the <code class="inline-code">LLM_PROVIDER</code> environment variable to easily switch between providers during testing and deployment.</li>
<li>Study how <code class="inline-code">DefaultAzureCredential</code> simplifies authentication in Azure-hosted environments, especially for multi-service applications.</li>
<li>Leverage the modular structure to add new providers by creating a new file and updating the <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function in <code class="inline-code">index.ts</code>.</li>
</ul>
<h2>Try This</h2>
<p>Challenge yourself to deepen your understanding:</p>
<ol>
<li><p><strong>Add a New Provider</strong>: Implement a new provider for an alternative LLM service (e.g., Hugging Face Inference API). Follow the existing pattern to create a new file and integrate it into the <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function in <code class="inline-code">index.ts</code>.</p>
</li>
<li><p><strong>Trace Provider Initialization</strong>: Add <code class="inline-code">console.log</code> statements to trace the initialization of each provider. Observe how the system dynamically selects and configures the provider based on the environment.</p>
</li>
<li><p><strong>Enhance Error Messages</strong>: Modify the error handling in <code class="inline-code">index.ts</code> to suggest valid <code class="inline-code">LLM_PROVIDER</code> options when an invalid value is detected. This will improve developer experience and reduce debugging time.</p>
</li>
</ol>
<hr>
<p>Excellent work! Continue to the next quest to uncover more mysteries of the starship&#39;s systems.</p>
<p>Mission accomplished, star navigator‚Äîyour Stellar LLM Provider Strategy has launched successfully, propelling you 20% closer to your cosmic conquest of knowledge! üöÄ‚≠êüì°</p>

</div>


      <div class="quest-navigation quest-navigation-bottom">
        <a href="quest-1.html" class="prev-quest-btn">‚Üê Previous: Quest 1</a>
        <a href="quest-3.html" class="next-quest-btn">Next: Quest 3 ‚Üí</a>
      </div>
    
    </div>
    
    <footer class="footer">
        <div class="footer-content">
            <span>Created using <a href="https://github.com/DanWahlin/ai-repo-adventures" target="_blank" rel="noopener noreferrer" class="repo-link">AI Repo Adventures</a></span>
        </div>
    </footer>
    
    <!-- Quest Navigator Script (for navbar Quests button functionality) -->
    <script src="../assets/shared/quest-navigator.js"></script>
    <!-- Theme Toggle Script (for light/dark mode toggle) -->
    <script src="../assets/theme-toggle.js"></script>
</body>
</html>