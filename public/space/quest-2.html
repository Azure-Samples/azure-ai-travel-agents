<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Navigating LLM Nebulas - Repo Adventure</title>
    <link rel="stylesheet" href="assets/theme.css">
    <link rel="stylesheet" href="../assets/shared/quest-navigator.css">
    <link rel="stylesheet" href="../assets/theme-toggle.css">
    <!-- Prism.js for syntax highlighting -->
    <!-- Using minimal theme since we override all styles with our custom theme -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script>
        // Configure Prism autoloader for syntax highlighting
        if (window.Prism && window.Prism.plugins && window.Prism.plugins.autoloader) {
            window.Prism.plugins.autoloader.languages_path = 'https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/';
        }

        // Trigger Prism highlighting after page load
        document.addEventListener('DOMContentLoaded', function() {
            if (window.Prism) {
                window.Prism.highlightAll();
            }
        });
    </script>
</head>
<body class="theme-space">

    <nav class="navbar">
        <div class="nav-content">
            <div class="nav-left">
                <a href="https://github.com/Azure-Samples/azure-ai-travel-agents" target="_blank" rel="noopener noreferrer" class="github-link">
                    <img src="../assets/shared/github-mark-white.svg" alt="GitHub" width="24" height="24">
                </a>
                <a href="index.html">Galactic Alliance Mission Control</a>
            </div>
            <div class="nav-middle">
            </div>
            <div class="nav-right">
                <a href="../index.html" class="nav-link">Change Adventure</a>
                <a href="#" class="nav-link quest-map-trigger">Quests</a>
                <button class="theme-toggle-btn" aria-label="Switch to light mode" type="button">
                    <div class="theme-toggle-slider">
                        <svg class="toggle-icon sun-icon" viewBox="0 0 24 24">
                            <circle cx="12" cy="12" r="4" fill="currentColor"/>
                            <path d="M12 2v2M12 20v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M2 12h2M20 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                        </svg>
                        <svg class="toggle-icon moon-icon" viewBox="0 0 24 24">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                        </svg>
                    </div>
                </button>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="quest-content">
    <h1>Quest 2: Flexible LLM Provider Strategy</h1>
<hr>
<p>As your starship ventures deeper into the LLM Nebulas, you encounter cosmic turbulence that demands adaptive machine learning capabilities. The Galactic Alliance&#39;s AI systems must dynamically switch between various LLM providers based on mission parameters and environmental constraints. Your task is to navigate this complexity by implementing and analyzing the flexible LLM provider strategy that powers the Alliance‚Äôs multi-agent workflows. With tools like Azure Managed Identity, GitHub Models, and Ollama configurations, you&#39;ll ensure the starship maintains its cutting-edge adaptability in the face of galactic uncertainty.</p>
<h2>Key Takeaways</h2>
<p>After completing this quest, you will understand:</p>
<ul>
<li>üéØ <strong>Dynamic Provider Switching</strong>: How to configure and switch between multiple LLM providers based on runtime conditions.</li>
<li>üîç <strong>Azure Managed Identity</strong>: How to securely authenticate with Azure services using managed credentials.</li>
<li>‚ö° <strong>Environment-Driven Logic</strong>: How environment variables dictate provider selection and configuration.</li>
<li>üí° <strong>Extensible Architecture</strong>: How modular provider design enables easy integration of new LLM systems.</li>
</ul>
<h2>File Exploration</h2>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/index.ts</code></a></h3>
<p>This file acts as the central hub for LLM provider management, dynamically selecting and configuring the appropriate provider based on the environment. It uses a factory function <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> to initialize providers like Azure OpenAI, GitHub Models, and Ollama Models. The modular design allows seamless integration of new providers while maintaining strict error handling for unknown configurations.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> factory function: Dynamically selects and initializes LLM providers based on environment variables.</li>
<li><code class="inline-code">LLMProvider</code> type: Enumerates valid provider options, ensuring type safety and clarity.</li>
<li>Error handling: Validates provider configuration and throws descriptive errors for unsupported types.</li>
<li>Environment-driven logic: Detects runtime conditions to determine the appropriate provider.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">import dotenv from &quot;dotenv&quot;;
dotenv.config();

import { llm as azureOpenAI } from &quot;./azure-openai.js&quot;;
import { llm as foundryLocal } from &quot;./foundry-local.js&quot;;
import { llm as githubModels } from &quot;./github-models.js&quot;;
import { llm as dockerModels } from &quot;./docker-models.js&quot;;
import { llm as ollamaModels } from &quot;./ollama-models.js&quot;;

type LLMProvider =
  | &quot;azure-openai&quot;
  | &quot;github-models&quot;
  | &quot;foundry-local&quot;
  | &quot;docker-models&quot;
  | &quot;ollama-models&quot;;

const provider = (process.env.LLM_PROVIDER || &quot;&quot;) as LLMProvider;

export const llm = async () =&gt; {
  switch (provider) {
    case &quot;azure-openai&quot;:
      return azureOpenAI();
    case &quot;github-models&quot;:
      return githubModels();
    case &quot;docker-models&quot;:
      return dockerModels();
    case &quot;ollama-models&quot;:
      return ollamaModels();
    case &quot;foundry-local&quot;:
      return foundryLocal();
    default:
      throw new Error(
        `Unknown LLM_PROVIDER &quot;${provider}&quot;. Valid options are: azure-openai, github-models, foundry-local, docker-models, ollama-models.`
      );
  }
};
</code></pre>
<ul>
<li>This code dynamically selects the LLM provider based on the <code class="inline-code">LLM_PROVIDER</code> environment variable.</li>
<li>The <code class="inline-code">LLMProvider</code> type ensures that only valid options are used, reducing runtime errors.</li>
<li>The switch-case structure simplifies provider selection while maintaining extensibility.</li>
<li>Error handling provides clear feedback when invalid configurations are encountered.</li>
<li>Modular imports allow easy addition of new providers without altering the core logic.</li>
</ul>
<hr>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/azure-openai.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/azure-openai.ts</code></a></h3>
<p>This file implements the Azure OpenAI provider, leveraging Azure Managed Identity for secure authentication in production environments. It also supports API key-based authentication for local Docker setups, ensuring compatibility across deployment scenarios.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> with environment detection: Adapts authentication strategy based on runtime conditions.</li>
<li><code class="inline-code">DefaultAzureCredential</code> and <code class="inline-code">ManagedIdentityCredential</code>: Enables secure access to Azure services.</li>
<li><code class="inline-code">getBearerTokenProvider()</code>: Retrieves tokens for Azure Cognitive Services, ensuring secure API calls.</li>
<li>API key fallback: Provides an alternative authentication method for local development.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">import { openai } from &quot;llamaindex&quot;;
import {
  DefaultAzureCredential,
  getBearerTokenProvider,
  ManagedIdentityCredential,
} from &quot;@azure/identity&quot;;

const AZURE_COGNITIVE_SERVICES_SCOPE =
  &quot;https://cognitiveservices.azure.com/.default&quot;;

export const llm = async () =&gt; {
  console.log(&quot;Using Azure OpenAI&quot;);

  const isRunningInLocalDocker = process.env.IS_LOCAL_DOCKER_ENV === &quot;true&quot;;
  
  if (isRunningInLocalDocker) {
    console.log(
      &quot;Running in local Docker environment, Azure Managed Identity is not supported. Authenticating with apiKey.&quot;
    );
    
    return openai({
      azure: {
        endpoint: process.env.AZURE_OPENAI_ENDPOINT,
        deployment: process.env.AZURE_OPENAI_DEPLOYMENT,
        apiKey: process.env.AZURE_OPENAI_API_KEY,
      },
    });
  }
  
  let credential: any = new DefaultAzureCredential();
  const clientId = process.env.AZURE_CLIENT_ID;
  if (clientId) {
    console.log(&quot;Using Azure Client ID:&quot;, clientId);
    credential = new ManagedIdentityCredential({
      clientId,
    });
  }

  const azureADTokenProvider = getBearerTokenProvider(
    credential,
    AZURE_COGNITIVE_SERVICES_SCOPE
  );

  return openai({
    azure: {
      azureADTokenProvider,
      endpoint: process.env.AZURE_OPENAI_ENDPOINT,
      deployment: process.env.AZURE_OPENAI_DEPLOYMENT,
    },
  });
};
</code></pre>
<ul>
<li>The <code class="inline-code">DefaultAzureCredential</code> simplifies authentication by handling multiple identity types.</li>
<li>The <code class="inline-code">ManagedIdentityCredential</code> provides secure, production-grade authentication for Azure services.</li>
<li>The <code class="inline-code">getBearerTokenProvider()</code> ensures tokens are scoped correctly for Azure Cognitive Services.</li>
<li>Environment detection enables flexible authentication strategies for local and production environments.</li>
<li>API key support ensures compatibility with local Docker setups, reducing development friction.</li>
</ul>
<hr>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/ollama-models.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/ollama-models.ts</code></a></h3>
<p>This file integrates the Ollama Models provider, demonstrating how to use local endpoints and API keys for secure and efficient communication. It also includes a workaround for tool call support in non-OpenAI providers.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> configuration: Sets up Ollama Models with local endpoint and API key authentication.</li>
<li>Tool call support: Adds compatibility for LlamaIndex tool calls, enhancing provider functionality.</li>
<li>Modular design: Ensures the provider can be easily integrated into the broader system.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">import { OpenAI, openai } from &quot;llamaindex&quot;;

export const llm = async () =&gt; {
  console.log(&quot;Using Ollama Models&quot;);
  const provider = openai({
    baseURL: process.env.OLLAMA_MODEL_ENDPOINT,
    apiKey: &#39;OLLAMA_API_KEY&#39;,
    model: process.env.OLLAMA_MODEL,
  });
  return {
    ...provider,
    // TODO: Remove this when LlamaIndex supports tool calls for non-OpenAI providers
    supportToolCall: true,
  } as OpenAI
};
</code></pre>
<ul>
<li>The <code class="inline-code">baseURL</code> enables integration with locally hosted Ollama endpoints.</li>
<li>API key authentication ensures secure access to the provider&#39;s services.</li>
<li>The <code class="inline-code">supportToolCall</code> workaround enhances compatibility with LlamaIndex workflows.</li>
<li>Modular design allows the provider to be easily added to the system without disrupting existing functionality.</li>
<li>This setup demonstrates how to adapt LLM configurations for specialized use cases.</li>
</ul>
<hr>
<h2>Helpful Hints</h2>
<ul>
<li>Use environment variables to toggle between LLM providers without modifying code.</li>
<li>Study the <code class="inline-code">DefaultAzureCredential</code> to understand how Azure handles authentication for different environments.</li>
<li>Analyze the <code class="inline-code">supportToolCall</code> workaround to learn how to extend compatibility in multi-agent systems.</li>
</ul>
<h2>Try This</h2>
<p>Challenge yourself to deepen your understanding:</p>
<ol>
<li><strong>Add a New Provider</strong>: Implement a new LLM provider in the <code class="inline-code">providers</code> directory. Use environment variables to configure its base URL and authentication method, and integrate it into the <code class="inline-code">index.ts</code> factory function.</li>
<li><strong>Trace Provider Selection</strong>: Add logging statements in the <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function to trace the provider selection process. Observe how environment variables impact the chosen configuration during runtime.</li>
<li><strong>Enhance Error Handling</strong>: Modify the error handling in <code class="inline-code">index.ts</code> to suggest valid provider options when an invalid configuration is detected. This improves developer experience and reduces debugging time.</li>
</ol>
<hr>
<p>Excellent work! Continue to the next quest to uncover more mysteries of the Galactic Alliance‚Äôs AI systems.</p>
<p>Mission accomplished, Star Navigator! üöÄ You&#39;ve charted the first sector of the LLM Nebulas with cosmic precision‚Äîkeep your thrusters steady and aim for the stars ahead! ‚≠ê‚ö°üì°</p>

</div>


      <div class="quest-navigation quest-navigation-bottom">
        <a href="quest-1.html" class="prev-quest-btn">‚Üê Previous: Quest 1</a>
        <a href="quest-3.html" class="next-quest-btn">Next: Quest 3 ‚Üí</a>
      </div>
    
    </div>
    
    <footer class="footer">
        <div class="footer-content">
            <span>Created using <a href="https://github.com/DanWahlin/ai-repo-adventures" target="_blank" rel="noopener noreferrer" class="repo-link">AI Repo Adventures</a></span>
        </div>
    </footer>
    
    <!-- Quest Navigator Script (for navbar Quests button functionality) -->
    <script src="../assets/shared/quest-navigator.js"></script>
    <!-- Theme Toggle Script (for light/dark mode toggle) -->
    <script src="../assets/theme-toggle.js"></script>
</body>
</html>