<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Navigating Dynamic LLM Providers - Repo Adventure</title>
    <link rel="stylesheet" href="assets/theme.css">
    <link rel="stylesheet" href="../assets/shared/quest-navigator.css">
    <link rel="stylesheet" href="../assets/theme-toggle.css">
    <!-- Prism.js for syntax highlighting -->
    <!-- Using minimal theme since we override all styles with our custom theme -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script>
        // Configure Prism autoloader for syntax highlighting
        if (window.Prism && window.Prism.plugins && window.Prism.plugins.autoloader) {
            window.Prism.plugins.autoloader.languages_path = 'https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/';
        }

        // Trigger Prism highlighting after page load
        document.addEventListener('DOMContentLoaded', function() {
            if (window.Prism) {
                window.Prism.highlightAll();
            }
        });
    </script>
</head>
<body class="theme-space">

    <nav class="navbar">
        <div class="nav-content">
            <div class="nav-left">
                <a href="https://github.com/Azure-Samples/azure-ai-travel-agents" target="_blank" rel="noopener noreferrer" class="github-link">
                    <img src="../assets/shared/github-mark-white.svg" alt="GitHub" width="24" height="24">
                </a>
                <a href="index.html">Galactic Mission: The Stellar Codex</a>
            </div>
            <div class="nav-middle">
            </div>
            <div class="nav-right">
                <a href="../index.html" class="nav-link">Change Theme</a>
                <a href="#" class="nav-link quest-map-trigger">Quests</a>
                <button class="theme-toggle-btn" aria-label="Switch to light mode" type="button">
                    <div class="theme-toggle-slider">
                        <svg class="toggle-icon sun-icon" viewBox="0 0 24 24">
                            <circle cx="12" cy="12" r="4" fill="currentColor"/>
                            <path d="M12 2v2M12 20v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M2 12h2M20 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                        </svg>
                        <svg class="toggle-icon moon-icon" viewBox="0 0 24 24">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                        </svg>
                    </div>
                </button>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="quest-content">
    <h1>Quest 2: Navigating Dynamic LLM Providers</h1>
<hr>
<p>As the <em>Azure Voyager</em> sails through the vast expanse of the galaxy, the Stellar Codex faces a critical challenge: dynamically adapting to the ever-changing nebula of LLM (Large Language Model) providers. To ensure seamless communication with the ship‚Äôs agents, you must configure and optimize the LlamaIndex Core to navigate between Azure OpenAI, GitHub Models, and Ollama Models. The success of this mission determines whether the crew can harness the full potential of their tools amidst the cosmic chaos of LLM diversity.</p>
<h2>Key Takeaways</h2>
<p>After completing this quest, you will understand:</p>
<ul>
<li>üéØ <strong>Dynamic LLM Selection</strong>: How to configure the system to support multiple LLM providers based on runtime environment variables.</li>
<li>üîç <strong>Azure Managed Identity Integration</strong>: How the system leverages Azure‚Äôs identity services to securely authenticate API calls.</li>
<li>‚ö° <strong>Error Handling in Provider Switching</strong>: How the code ensures robustness when encountering unknown or unsupported providers.</li>
<li>üí° <strong>Modular LLM Configuration</strong>: How to structure LLM provider integrations for scalability and maintainability.</li>
</ul>
<h2>File Exploration</h2>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/index.ts</code></a></h3>
<p>This file serves as the command center for managing LLM provider configurations. It dynamically selects the appropriate provider based on the <code class="inline-code">LLM_PROVIDER</code> environment variable, ensuring that the system can seamlessly switch between multiple LLM backends. The modular design facilitates the addition of new providers without disrupting existing functionality.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> dynamically selects and initializes the appropriate LLM provider at runtime using a switch statement.</li>
<li><code class="inline-code">LLMProvider</code> defines valid provider options, ensuring type safety and clarity.</li>
<li>Error handling ensures unsupported providers are rejected with a clear message.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">import dotenv from &quot;dotenv&quot;;
dotenv.config();

import { llm as azureOpenAI } from &quot;./azure-openai.js&quot;;
import { llm as foundryLocal } from &quot;./foundry-local.js&quot;;
import { llm as githubModels } from &quot;./github-models.js&quot;;
import { llm as dockerModels } from &quot;./docker-models.js&quot;;
import { llm as ollamaModels } from &quot;./ollama-models.js&quot;;

type LLMProvider =
  | &quot;azure-openai&quot;
  | &quot;github-models&quot;
  | &quot;foundry-local&quot;
  | &quot;docker-models&quot;
  | &quot;ollama-models&quot;;

const provider = (process.env.LLM_PROVIDER || &quot;&quot;) as LLMProvider;

export const llm = async () =&gt; {
  switch (provider) {
    case &quot;azure-openai&quot;:
      return azureOpenAI();
    case &quot;github-models&quot;:
      return githubModels();
    case &quot;docker-models&quot;:
      return dockerModels();
    case &quot;ollama-models&quot;:
      return ollamaModels();
    case &quot;foundry-local&quot;:
      return foundryLocal();
    default:
      throw new Error(
        `Unknown LLM_PROVIDER &quot;${provider}&quot;. Valid options are: azure-openai, github-models, foundry-local, docker-models, ollama-models.`
      );
  }
};
</code></pre>
<ul>
<li>The <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function dynamically initializes the correct provider based on the <code class="inline-code">LLM_PROVIDER</code> environment variable.</li>
<li>The <code class="inline-code">LLMProvider</code> type ensures that only valid provider names are used, reducing potential runtime errors.</li>
<li>The default case in the switch statement provides robust error handling, ensuring clarity when an unsupported provider is specified.</li>
<li>This modular approach makes it easy to add new providers by simply importing and including them in the switch statement.</li>
</ul>
<hr>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/azure-openai.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/azure-openai.ts</code></a></h3>
<p>This file handles the configuration for Azure OpenAI, supporting both local and production environments. It uses Azure‚Äôs Managed Identity for secure authentication in production, while falling back to an API key for local Docker environments.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> dynamically configures Azure OpenAI with either Managed Identity or API key authentication.</li>
<li><code class="inline-code">DefaultAzureCredential</code> and <code class="inline-code">ManagedIdentityCredential</code> are used to support secure, scalable authentication.</li>
<li>The <code class="inline-code">getBearerTokenProvider()</code> function provides an abstraction for token-based authentication.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">import { openai } from &quot;llamaindex&quot;;
import {
  DefaultAzureCredential,
  getBearerTokenProvider,
  ManagedIdentityCredential,
} from &quot;@azure/identity&quot;;

const AZURE_COGNITIVE_SERVICES_SCOPE =
  &quot;https://cognitiveservices.azure.com/.default&quot;;

export const llm = async () =&gt; {
  console.log(&quot;Using Azure OpenAI&quot;);

  const isRunningInLocalDocker = process.env.IS_LOCAL_DOCKER_ENV === &quot;true&quot;;
  
  if (isRunningInLocalDocker) {
    // running in local Docker environment
    console.log(
      &quot;Running in local Docker environment, Azure Managed Identity is not supported. Authenticating with apiKey.&quot;
    );
    
    return openai({
      azure: {
        endpoint: process.env.AZURE_OPENAI_ENDPOINT,
        deployment: process.env.AZURE_OPENAI_DEPLOYMENT,
        apiKey: process.env.AZURE_OPENAI_API_KEY,
      },
    });
  }
  
  let credential: any = new DefaultAzureCredential();
  const clientId = process.env.AZURE_CLIENT_ID;
  if (clientId) {
    // running in production with a specific client ID
    console.log(&quot;Using Azure Client ID:&quot;, clientId);
    credential = new ManagedIdentityCredential({
      clientId,
    });
  }

  const azureADTokenProvider = getBearerTokenProvider(
    credential,
    AZURE_COGNITIVE_SERVICES_SCOPE
  );

  return openai({
    azure: {
      azureADTokenProvider,
      endpoint: process.env.AZURE_OPENAI_ENDPOINT,
      deployment: process.env.AZURE_OPENAI_DEPLOYMENT,
    },
  });
};
</code></pre>
<ul>
<li>The <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function adapts to different environments, using API key authentication for local Docker setups and Managed Identity for production environments.</li>
<li>The <code class="inline-code">DefaultAzureCredential</code> and <code class="inline-code">ManagedIdentityCredential</code> classes abstract away the complexity of Azure authentication, making the code more maintainable.</li>
<li>The <code class="inline-code">getBearerTokenProvider()</code> function encapsulates token acquisition, ensuring secure and efficient API calls.</li>
<li>This design balances local development flexibility with production-grade security.</li>
</ul>
<hr>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/github-models.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/github-models.ts</code></a></h3>
<p>This file configures GitHub-hosted models, demonstrating a straightforward setup with a static API key and model parameters.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> configures GitHub Models with a base URL, API key, and model name.</li>
<li>Environment variables are used to decouple sensitive information from the codebase.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">import { openai } from &quot;llamaindex&quot;;

export const llm = async () =&gt; {
  console.log(&quot;Using GitHub Models&quot;);
  return openai({
    baseURL: &quot;https://models.inference.ai.azure.com&quot;,
    apiKey: process.env.GITHUB_TOKEN,
    model: process.env.GITHUB_MODEL,
  });
};
</code></pre>
<ul>
<li>The <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function provides a minimal yet effective configuration for GitHub Models.</li>
<li>The use of environment variables ensures sensitive information like API keys is not hardcoded, enhancing security.</li>
<li>The modular structure allows this provider to be easily swapped or updated without impacting other parts of the system.</li>
</ul>
<hr>
<h2>Helpful Hints</h2>
<ul>
<li>Investigate how the <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function in <code class="inline-code">index.ts</code> dynamically selects providers and ensure you understand the error handling logic for unsupported providers.</li>
<li>Explore the differences between Azure OpenAI‚Äôs authentication methods and consider how Managed Identity simplifies production deployments.</li>
<li>Look into the use of environment variables across all provider files to understand how they enhance security and configurability.</li>
</ul>
<h2>Try This</h2>
<p>Challenge yourself to deepen your understanding:</p>
<ol>
<li><p><strong>Add a New Provider</strong>: Implement a new provider configuration (e.g., for Hugging Face models). Follow the pattern in <code class="inline-code">src/api/src/orchestrator/llamaindex/providers/</code> and register it in <code class="inline-code">index.ts</code>.</p>
<ul>
<li>Example: Use the Hugging Face Inference API with an API key and model name.</li>
</ul>
</li>
<li><p><strong>Trace Authentication Flow</strong>: Add logging to the Azure OpenAI <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function to trace how credentials are selected and tokens are obtained. Test this in both local and production-like environments.</p>
</li>
<li><p><strong>Enhance Error Messaging</strong>: Modify the error handling in <code class="inline-code">index.ts</code> to suggest valid <code class="inline-code">LLM_PROVIDER</code> options when an unsupported value is encountered. This will improve developer experience.</p>
</li>
</ol>
<hr>
<p>Excellent work! Continue to the next quest to uncover more mysteries of the <em>Azure Voyager</em>&#39;s systems.</p>
<p>Mission accomplished, star navigator! You&#39;ve charted a brilliant course through the cosmic maze of dynamic LLM providers‚Äîkeep your thrusters steady as you rocket toward the next stellar milestone! üöÄ‚≠êüì°</p>

</div>


      <div class="quest-navigation quest-navigation-bottom">
        <a href="quest-1.html" class="prev-quest-btn">‚Üê Previous: Quest 1</a>
        <a href="quest-3.html" class="next-quest-btn">Next: Quest 3 ‚Üí</a>
      </div>
    
    </div>
    
    <footer class="footer">
        <div class="footer-content">
            <span>Created using <a href="https://github.com/DanWahlin/ai-repo-adventures" target="_blank" rel="noopener noreferrer" class="repo-link">AI Repo Adventures</a></span>
        </div>
    </footer>
    
    <!-- Quest Navigator Script (for navbar Quests button functionality) -->
    <script src="../assets/shared/quest-navigator.js"></script>
    <!-- Theme Toggle Script (for light/dark mode toggle) -->
    <script src="../assets/theme-toggle.js"></script>
</body>
</html>