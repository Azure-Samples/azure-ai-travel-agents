<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Mystic LLM Forge - Repo Adventure</title>
    <link rel="stylesheet" href="assets/theme.css">
    <link rel="stylesheet" href="../assets/shared/quest-navigator.css">
    <link rel="stylesheet" href="../assets/theme-toggle.css">
    <!-- Prism.js for syntax highlighting -->
    <!-- Using minimal theme since we override all styles with our custom theme -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script>
        // Configure Prism autoloader for syntax highlighting
        if (window.Prism && window.Prism.plugins && window.Prism.plugins.autoloader) {
            window.Prism.plugins.autoloader.languages_path = 'https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/';
        }

        // Trigger Prism highlighting after page load
        document.addEventListener('DOMContentLoaded', function() {
            if (window.Prism) {
                window.Prism.highlightAll();
            }
        });
    </script>
</head>
<body class="theme-mythical">

    <nav class="navbar">
        <div class="nav-content">
            <div class="nav-left">
                <a href="https://github.com/Azure-Samples/azure-ai-travel-agents" target="_blank" rel="noopener noreferrer" class="github-link">
                    <img src="../assets/shared/github-mark.svg" alt="GitHub" width="24" height="24">
                </a>
                <a href="index.html">The Enchanted Kingdom of Azure AI Agents</a>
            </div>
            <div class="nav-middle">
            </div>
            <div class="nav-right">
                <a href="../index.html" class="nav-link">Change Theme</a>
                <a href="#" class="nav-link quest-map-trigger">Quests</a>
                <button class="theme-toggle-btn" aria-label="Switch to light mode" type="button">
                    <div class="theme-toggle-slider">
                        <svg class="toggle-icon sun-icon" viewBox="0 0 24 24">
                            <circle cx="12" cy="12" r="4" fill="currentColor"/>
                            <path d="M12 2v2M12 20v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M2 12h2M20 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                        </svg>
                        <svg class="toggle-icon moon-icon" viewBox="0 0 24 24">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                        </svg>
                    </div>
                </button>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="quest-content">
    <h1>Quest 2: The Mystic LLM Forge</h1>
<hr>
<p>In the heart of the Kingdom of Azure lies the Mystic LLM Forge, where enchanted models are crafted to power the kingdom&#39;s agents. The Forge, overseen by the Grand Mage LlamaIndex, selects the finest tools from Azure, GitHub, and Ollama to imbue the agents with their magical abilities. Yet, the Forge&#39;s secrets remain hidden, and only the bravest adventurers can decipher its intricate spells and unlock its full potential.</p>
<h2>Key Takeaways</h2>
<p>After completing this quest, you will understand:</p>
<ul>
<li>üéØ <strong>Flexible LLM Provider Strategy</strong>: How environment-based configuration enables dynamic selection of LLM providers.</li>
<li>üîç <strong>Azure Credential Management</strong>: How <code class="inline-code">DefaultAzureCredential</code> and <code class="inline-code">ManagedIdentityCredential</code> integrate with Azure OpenAI services.</li>
<li>‚ö° <strong>Error Handling in Provider Selection</strong>: How robust error handling ensures stability when invalid configurations are encountered.</li>
<li>üí° <strong>Provider Extensibility</strong>: How the modular design allows adding new LLM providers with minimal changes.</li>
</ul>
<h2>File Exploration</h2>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/index.ts</code></a></h3>
<p>The Forge&#39;s index file acts as the spellbook for LLM provider selection, dynamically invoking the correct provider based on environmental configurations. This file demonstrates how the system supports multiple providers while maintaining a clear and extensible architecture. It uses a factory function to initialize the appropriate LLM provider and includes defensive programming to handle invalid configurations gracefully.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> factory function dynamically selects the correct LLM provider based on <code class="inline-code">process.env.LLM_PROVIDER</code>.</li>
<li><code class="inline-code">LLMProvider</code> type defines valid provider options, ensuring type safety.</li>
<li>Defensive error handling prevents runtime failures when an invalid provider is configured.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">import dotenv from &quot;dotenv&quot;;
dotenv.config();

import { llm as azureOpenAI } from &quot;./azure-openai.js&quot;;
import { llm as foundryLocal } from &quot;./foundry-local.js&quot;;
import { llm as githubModels } from &quot;./github-models.js&quot;;
import { llm as dockerModels } from &quot;./docker-models.js&quot;;
import { llm as ollamaModels } from &quot;./ollama-models.js&quot;;

type LLMProvider =
  | &quot;azure-openai&quot;
  | &quot;github-models&quot;
  | &quot;foundry-local&quot;
  | &quot;docker-models&quot;
  | &quot;ollama-models&quot;;

const provider = (process.env.LLM_PROVIDER || &quot;&quot;) as LLMProvider;

export const llm = async () =&gt; {
  switch (provider) {
    case &quot;azure-openai&quot;:
      return azureOpenAI();
    case &quot;github-models&quot;:
      return githubModels();
    case &quot;docker-models&quot;:
      return dockerModels();
    case &quot;ollama-models&quot;:
      return ollamaModels();
    case &quot;foundry-local&quot;:
      return foundryLocal();
    default:
      throw new Error(
        `Unknown LLM_PROVIDER &quot;${provider}&quot;. Valid options are: azure-openai, github-models, foundry-local, docker-models, ollama-models.`
      );
  }
};
</code></pre>
<ul>
<li>The <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function uses a switch statement to map environment variables to specific providers, ensuring dynamic configuration.</li>
<li>Type safety is enforced with the <code class="inline-code">LLMProvider</code> union type, reducing the risk of invalid inputs.</li>
<li>Error handling provides clear feedback when an unsupported provider is configured, aiding debugging and system stability.</li>
<li>Modular imports allow new providers to be added without impacting existing functionality.</li>
<li>The use of <code class="inline-code">dotenv</code> ensures environment variables are loaded securely and consistently.</li>
</ul>
<hr>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/azure-openai.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/azure-openai.ts</code></a></h3>
<p>This file reveals the spell for invoking Azure OpenAI services, leveraging Azure&#39;s identity management and token generation capabilities. It showcases how the system adapts to different deployment environments, whether local Docker or production.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function configures Azure OpenAI with either <code class="inline-code">apiKey</code> or <code class="inline-code">ManagedIdentityCredential</code> depending on the deployment environment.</li>
<li><code class="inline-code">DefaultAzureCredential</code> integrates seamlessly with Azure AD for production environments.</li>
<li><code class="inline-code">getBearerTokenProvider()</code> generates tokens for Azure Cognitive Services, enabling secure authentication.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">import { openai } from &quot;llamaindex&quot;;
import {
  DefaultAzureCredential,
  getBearerTokenProvider,
  ManagedIdentityCredential,
} from &quot;@azure/identity&quot;;

const AZURE_COGNITIVE_SERVICES_SCOPE =
  &quot;https://cognitiveservices.azure.com/.default&quot;;

export const llm = async () =&gt; {
  console.log(&quot;Using Azure OpenAI&quot;);

  const isRunningInLocalDocker = process.env.IS_LOCAL_DOCKER_ENV === &quot;true&quot;;
  
  if (isRunningInLocalDocker) {
    console.log(
      &quot;Running in local Docker environment, Azure Managed Identity is not supported. Authenticating with apiKey.&quot;
    );
    return openai({
      azure: {
        endpoint: process.env.AZURE_OPENAI_ENDPOINT,
        deployment: process.env.AZURE_OPENAI_DEPLOYMENT,
        apiKey: process.env.AZURE_OPENAI_API_KEY,
      },
    });
  }
  
  let credential: any = new DefaultAzureCredential();
  const clientId = process.env.AZURE_CLIENT_ID;
  if (clientId) {
    console.log(&quot;Using Azure Client ID:&quot;, clientId);
    credential = new ManagedIdentityCredential({
      clientId,
    });
  }

  const azureADTokenProvider = getBearerTokenProvider(
    credential,
    AZURE_COGNITIVE_SERVICES_SCOPE
  );

  return openai({
    azure: {
      azureADTokenProvider,
      endpoint: process.env.AZURE_OPENAI_ENDPOINT,
      deployment: process.env.AZURE_OPENAI_DEPLOYMENT,
    },
  });
};
</code></pre>
<ul>
<li>The <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function adapts to local and production environments, ensuring flexibility in deployment scenarios.</li>
<li><code class="inline-code">DefaultAzureCredential</code> simplifies authentication in production, supporting multiple Azure identity types.</li>
<li><code class="inline-code">ManagedIdentityCredential</code> allows fine-grained control over authentication when a specific client ID is required.</li>
<li>The <code class="inline-code">getBearerTokenProvider()</code> function securely generates tokens for Azure Cognitive Services, enabling API calls without exposing sensitive credentials.</li>
<li>Environment-based conditional logic ensures compatibility across diverse deployment setups.</li>
</ul>
<hr>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/github-models.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/github-models.ts</code></a></h3>
<p>This file demonstrates the invocation of GitHub-hosted models, showcasing a minimal configuration approach for external model providers.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function configures GitHub Models with an API key and model name.</li>
<li>Simplified provider setup ensures ease of integration with external services.</li>
<li>Base URL configuration centralizes endpoint management for GitHub-hosted models.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">import { openai } from &quot;llamaindex&quot;;

export const llm = async () =&gt; {
  console.log(&quot;Using GitHub Models&quot;);
  return openai({
    baseURL: &quot;https://models.inference.ai.azure.com&quot;,
    apiKey: process.env.GITHUB_TOKEN,
    model: process.env.GITHUB_MODEL,
  });
};
</code></pre>
<ul>
<li>The <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function simplifies GitHub model integration by centralizing API key and model configuration.</li>
<li>The <code class="inline-code">baseURL</code> provides a single point of control for endpoint changes, enhancing maintainability.</li>
<li>Secure handling of API keys ensures sensitive credentials are not exposed in the codebase.</li>
<li>This minimal setup demonstrates how external providers can be integrated with minimal boilerplate code.</li>
<li>The approach highlights extensibility, allowing other external providers to follow similar patterns.</li>
</ul>
<hr>
<h2>Helpful Hints</h2>
<ul>
<li>Use the <code class="inline-code">dotenv</code> library to securely manage environment variables and avoid hardcoding sensitive credentials.</li>
<li>When adding new providers, follow the modular design pattern demonstrated in <code class="inline-code">index.ts</code> to maintain extensibility.</li>
<li>Study the conditional logic in <code class="inline-code">azure-openai.ts</code> to understand how deployment environments influence configuration.</li>
</ul>
<h2>Try This</h2>
<p>Challenge yourself to deepen your understanding:</p>
<ol>
<li><p><strong>Add a Custom Provider</strong>: Create a new LLM provider configuration in <code class="inline-code">src/api/src/orchestrator/llamaindex/providers/</code>. Use a mock API and integrate it into <code class="inline-code">index.ts</code> to test the modular architecture.</p>
<ul>
<li>Example: &quot;Configure a provider for a hypothetical service called &#39;MysticAI&#39;. Add environment variables for its API key and endpoint, and implement the logic to invoke its models.&quot;</li>
</ul>
</li>
<li><p><strong>Trace Azure Authentication Flow</strong>: Add <code class="inline-code">console.log</code> statements in <code class="inline-code">azure-openai.ts</code> to observe how credentials are generated and used during API calls. Analyze the differences between <code class="inline-code">DefaultAzureCredential</code> and <code class="inline-code">ManagedIdentityCredential</code>.</p>
<ul>
<li>Example: &quot;Track the token generation process and compare the outputs for local Docker and production environments.&quot;</li>
</ul>
</li>
<li><p><strong>Enhance Error Handling</strong>: Modify the error handling in <code class="inline-code">index.ts</code> to provide more detailed feedback when an unknown provider is selected. Include suggestions for valid options based on the <code class="inline-code">LLMProvider</code> type.</p>
<ul>
<li>Example: &quot;Instead of throwing a generic error, log the available provider names and guide developers to configure the correct environment variable.&quot;</li>
</ul>
</li>
</ol>
<hr>
<p>Excellent work! Continue to the next quest to uncover more mysteries of the enchanted codebase.</p>
<p>Behold, valiant seeker of wisdom, for you have ignited the first arcane ember of the Mystic LLM Forge, a feat worthy of legends‚Äîpress onward, for the realms of enlightenment await your triumphant return! ‚öîÔ∏è ‚≠ê üíé</p>

</div>


      <div class="quest-navigation quest-navigation-bottom">
        <a href="quest-1.html" class="prev-quest-btn">‚Üê Previous: Quest 1</a>
        <a href="quest-3.html" class="next-quest-btn">Next: Quest 3 ‚Üí</a>
      </div>
    
    </div>
    
    <footer class="footer">
        <div class="footer-content">
            <span>Created using <a href="https://github.com/DanWahlin/ai-repo-adventures" target="_blank" rel="noopener noreferrer" class="repo-link">AI Repo Adventures</a></span>
        </div>
    </footer>
    
    <!-- Quest Navigator Script (for navbar Quests button functionality) -->
    <script src="../assets/shared/quest-navigator.js"></script>
    <!-- Theme Toggle Script (for light/dark mode toggle) -->
    <script src="../assets/theme-toggle.js"></script>
</body>
</html>