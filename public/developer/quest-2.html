<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Flexible LLM Provider Strategy - Repo Adventure</title>
    <link rel="stylesheet" href="assets/theme.css">
    <link rel="stylesheet" href="../assets/shared/quest-navigator.css">
    <link rel="stylesheet" href="../assets/theme-toggle.css">
    <!-- Prism.js for syntax highlighting -->
    <!-- Using minimal theme since we override all styles with our custom theme -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script>
        // Configure Prism autoloader for syntax highlighting
        if (window.Prism && window.Prism.plugins && window.Prism.plugins.autoloader) {
            window.Prism.plugins.autoloader.languages_path = 'https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/';
        }
        
        // Trigger Prism highlighting after page load
        document.addEventListener('DOMContentLoaded', function() {
            if (window.Prism) {
                window.Prism.highlightAll();
            }
        });
    </script>
</head>
<body class="theme-developer">

    <nav class="navbar">
        <div class="nav-content">
            <div class="nav-left">
                <a href="https://github.com/Azure-Samples/azure-ai-travel-agents" target="_blank" rel="noopener noreferrer" class="github-link">
                    <img src="../assets/shared/github-mark.svg" alt="GitHub" width="24" height="24">
                </a>
                <a href="index.html">Multi-Agent Orchestration with Azure AI Travel Agents</a>
            </div>
            <div class="nav-middle">
            </div>
            <div class="nav-right">
                <a href="../index.html" class="nav-link">Change Theme</a>
                <a href="#" class="nav-link quest-map-trigger">Quests</a>
                <button class="theme-toggle-btn" aria-label="Switch to light mode" type="button">
                    <div class="theme-toggle-slider">
                        <svg class="toggle-icon sun-icon" viewBox="0 0 24 24">
                            <circle cx="12" cy="12" r="4" fill="currentColor"/>
                            <path d="M12 2v2M12 20v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M2 12h2M20 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                        </svg>
                        <svg class="toggle-icon moon-icon" viewBox="0 0 24 24">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                        </svg>
                    </div>
                </button>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="quest-content">
    <h1>Quest 2: Flexible LLM Provider Strategy</h1>
<hr>
<p>The Azure AI Travel Agents system employs a flexible architecture for integrating different large language model (LLM) providers. This quest explores how the application dynamically selects and configures LLM providers, including Azure OpenAI, GitHub-hosted models, and Ollama models. By understanding this strategy, developers can adapt the system to support additional providers or tailor configurations for specific environments.</p>
<h2>Quest Objectives</h2>
<p>As you explore the code below, investigate these key questions:</p>
<ul>
<li>üîç <strong>Provider Selection Logic</strong>: How does the system determine which LLM provider to use at runtime, and what are the fallback mechanisms?</li>
<li>‚ö° <strong>Azure Integration</strong>: What techniques are used to authenticate and configure Azure OpenAI in both local and production environments?</li>
<li>üõ°Ô∏è <strong>Error Handling</strong>: How does the system ensure robust error handling when an invalid or unsupported LLM provider is specified?</li>
</ul>
<h2>File Exploration</h2>
<h3><span class="header-prefix"><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts" target="_blank" rel="noopener noreferrer">src/api/src/orchestrator/llamaindex/providers/index.ts</a>:</span> LLM Provider Registry</h3>
<p>This file serves as the central registry for all supported LLM providers. It determines which provider to use based on the <code class="inline-code">LLM_PROVIDER</code> environment variable and delegates initialization to the corresponding provider module. The <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm</code></a> function acts as a factory method, ensuring that only valid providers are used.</p>
<h4>Highlights</h4>
<ul>
<li><code class="inline-code">LLMProvider</code> defines the valid provider options, ensuring type safety and preventing invalid configurations.</li>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm</code></a> dynamically selects and initializes the appropriate provider based on the <code class="inline-code">LLM_PROVIDER</code> environment variable.</li>
<li>Error handling ensures that an exception is thrown if an unsupported provider is specified, guiding developers to correct the configuration.</li>
</ul>
<h3><span class="header-prefix"><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/azure-openai.ts" target="_blank" rel="noopener noreferrer">src/api/src/orchestrator/llamaindex/providers/azure-openai.ts</a>:</span> Azure OpenAI Integration</h3>
<p>This file configures the Azure OpenAI provider, supporting both local development and production environments. It uses Azure&#39;s Managed Identity Credential for secure authentication in production while falling back to API key authentication for local setups.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm</code></a> initializes the Azure OpenAI provider, dynamically switching between authentication methods based on the environment.</li>
<li><code class="inline-code">DefaultAzureCredential</code> and <code class="inline-code">ManagedIdentityCredential</code> provide secure and flexible authentication options.</li>
<li><code class="inline-code">getBearerTokenProvider</code> generates a token provider for Azure Cognitive Services, enabling seamless integration with Azure APIs.</li>
</ul>
<h3><span class="header-prefix"><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/ollama-models.ts" target="_blank" rel="noopener noreferrer">src/api/src/orchestrator/llamaindex/providers/ollama-models.ts</a>:</span> Ollama Model Configuration</h3>
<p>This file demonstrates how to integrate an Ollama-hosted model. It highlights the flexibility of the system to support non-OpenAI providers by extending the <code class="inline-code">openai</code> configuration.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm</code></a> configures the Ollama provider with a custom endpoint and API key.</li>
<li>The <code class="inline-code">supportToolCall</code> property is added to enable compatibility with LlamaIndex, showcasing the extensibility of the architecture.</li>
<li>The code includes a placeholder for future updates, ensuring forward compatibility as the system evolves.</li>
</ul>
<h2>Code</h2>
<h3><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts" target="_blank" rel="noopener noreferrer">src/api/src/orchestrator/llamaindex/providers/index.ts</a></h3>
<pre><code class="language-typescript">import dotenv from &quot;dotenv&quot;;
dotenv.config();

import { llm as azureOpenAI } from &quot;./azure-openai.js&quot;;
import { llm as githubModels } from &quot;./github-models.js&quot;;
import { llm as dockerModels } from &quot;./docker-models.js&quot;;
import { llm as ollamaModels } from &quot;./ollama-models.js&quot;;

type LLMProvider =
  | &quot;azure-openai&quot;
  | &quot;github-models&quot;
  | &quot;foundry-local&quot;
  | &quot;docker-models&quot;
  | &quot;ollama-models&quot;;

const provider = (process.env.LLM_PROVIDER || &quot;&quot;) as LLMProvider;

export const llm = async () =&gt; {
  switch (provider) {
    case &quot;azure-openai&quot;:
      return azureOpenAI();
    case &quot;github-models&quot;:
      return githubModels();
    case &quot;docker-models&quot;:
      return dockerModels();
    case &quot;ollama-models&quot;:
      return ollamaModels();
    default:
      throw new Error(
        `Unknown LLM_PROVIDER &quot;${provider}&quot;. Valid options are: azure-openai, github-models, foundry-local, docker-models, ollama-models.`
      );
  }
};
</code></pre>
<ul>
<li>This code dynamically selects the LLM provider based on the <code class="inline-code">LLM_PROVIDER</code> environment variable.</li>
<li>The <code class="inline-code">LLMProvider</code> type ensures that only valid provider names are used, reducing the likelihood of misconfiguration.</li>
<li>The <code class="inline-code">default</code> case in the <code class="inline-code">switch</code> statement provides clear error messaging, helping developers quickly identify and resolve issues.</li>
</ul>
<hr>
<h3><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/azure-openai.ts" target="_blank" rel="noopener noreferrer">src/api/src/orchestrator/llamaindex/providers/azure-openai.ts</a></h3>
<pre><code class="language-typescript">import { openai } from &quot;llamaindex&quot;;
import {
  DefaultAzureCredential,
  getBearerTokenProvider,
  ManagedIdentityCredential,
} from &quot;@azure/identity&quot;;

const AZURE_COGNITIVE_SERVICES_SCOPE =
  &quot;https://cognitiveservices.azure.com/.default&quot;;

export const llm = async () =&gt; {
  console.log(&quot;Using Azure OpenAI&quot;);

  const isRunningInLocalDocker = process.env.IS_LOCAL_DOCKER_ENV === &quot;true&quot;;
  
  if (isRunningInLocalDocker) {
    console.log(
      &quot;Running in local Docker environment, Azure Managed Identity is not supported. Authenticating with apiKey.&quot;
    );
    return openai({
      azure: {
        endpoint: process.env.AZURE_OPENAI_ENDPOINT,
        deployment: process.env.AZURE_OPENAI_DEPLOYMENT,
        apiKey: process.env.AZURE_OPENAI_API_KEY,
      },
    });
  }
  
  let credential: any = new DefaultAzureCredential();
  const clientId = process.env.AZURE_CLIENT_ID;
  if (clientId) {
    console.log(&quot;Using Azure Client ID:&quot;, clientId);
    credential = new ManagedIdentityCredential({ clientId });
  }

  const azureADTokenProvider = getBearerTokenProvider(
    credential,
    AZURE_COGNITIVE_SERVICES_SCOPE
  );

  return openai({
    azure: {
      azureADTokenProvider,
      endpoint: process.env.AZURE_OPENAI_ENDPOINT,
      deployment: process.env.AZURE_OPENAI_DEPLOYMENT,
    },
  });
};
</code></pre>
<ul>
<li>This code configures Azure OpenAI with flexible authentication methods, supporting both local and production environments.</li>
<li>The <code class="inline-code">DefaultAzureCredential</code> and <code class="inline-code">ManagedIdentityCredential</code> classes simplify secure authentication in Azure-hosted environments.</li>
<li>The use of <code class="inline-code">process.env</code> for configuration ensures that sensitive values are not hardcoded, adhering to best practices.</li>
</ul>
<hr>
<h3><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/ollama-models.ts" target="_blank" rel="noopener noreferrer">src/api/src/orchestrator/llamaindex/providers/ollama-models.ts</a></h3>
<pre><code class="language-typescript">import { OpenAI, openai } from &quot;llamaindex&quot;;

export const llm = async () =&gt; {
  console.log(&quot;Using Ollama Models&quot;);
  const provider = openai({
    baseURL: process.env.OLLAMA_MODEL_ENDPOINT,
    apiKey: &#39;OLLAMA_API_KEY&#39;,
    model: process.env.OLLAMA_MODEL,
  });
  return {
    ...provider,
    supportToolCall: true,
  } as OpenAI;
};
</code></pre>
<ul>
<li>This code demonstrates how to configure a custom LLM provider, showcasing the extensibility of the system.</li>
<li>Adding <code class="inline-code">supportToolCall</code> ensures compatibility with LlamaIndex, enabling tool-based workflows.</li>
<li>The use of <code class="inline-code">process.env</code> allows for easy reconfiguration without modifying the code, supporting deployment flexibility.</li>
</ul>
<hr>
<h2>Helpful Hints</h2>
<ul>
<li>Review the <code class="inline-code">LLMProvider</code> type to understand the valid options for the <code class="inline-code">LLM_PROVIDER</code> environment variable.</li>
<li>Pay attention to how Azure authentication switches between API keys and Managed Identity based on the environment.</li>
<li>Explore the <code class="inline-code">ollama-models.ts</code> file to see how non-OpenAI providers can be integrated into the system.</li>
</ul>
<hr>
<p>Excellent work! Continue to the next quest to uncover more mysteries of the Azure AI Travel Agents system.</p>
<p>Quest &#39;Flexible LLM Provider Strategy&#39; successfully deployed‚Äîyour architecture is scaling with precision; keep iterating toward the 100% milestone! üöÄüíé‚ö°</p>

</div>


      <div class="quest-navigation quest-navigation-bottom">
        <a href="quest-1.html" class="prev-quest-btn">‚Üê Previous: Quest 1</a>
        <a href="quest-3.html" class="next-quest-btn">Next: Quest 3 ‚Üí</a>
      </div>
    
    </div>
    
    <footer class="footer">
        <div class="footer-content">
            <span>Created using <a href="https://github.com/DanWahlin/ai-repo-adventures" target="_blank" rel="noopener noreferrer" class="repo-link">AI Repo Adventures</a></span>
        </div>
    </footer>
    
    <!-- Quest Navigator Script (for navbar Quests button functionality) -->
    <script src="../assets/shared/quest-navigator.js"></script>
    <!-- Theme Toggle Script (for light/dark mode toggle) -->
    <script src="../assets/theme-toggle.js"></script>
</body>
</html>