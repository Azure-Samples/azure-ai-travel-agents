<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Flexible LLM Provider Strategy - Repo Adventure</title>
    <link rel="stylesheet" href="assets/theme.css">
    <link rel="stylesheet" href="../assets/shared/quest-navigator.css">
    <link rel="stylesheet" href="../assets/theme-toggle.css">
    <!-- Prism.js for syntax highlighting -->
    <!-- Using minimal theme since we override all styles with our custom theme -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script>
        // Configure Prism autoloader for syntax highlighting
        if (window.Prism && window.Prism.plugins && window.Prism.plugins.autoloader) {
            window.Prism.plugins.autoloader.languages_path = 'https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/';
        }

        // Trigger Prism highlighting after page load
        document.addEventListener('DOMContentLoaded', function() {
            if (window.Prism) {
                window.Prism.highlightAll();
            }
        });
    </script>
</head>
<body class="theme-developer">

    <nav class="navbar">
        <div class="nav-content">
            <div class="nav-left">
                <a href="https://github.com/Azure-Samples/azure-ai-travel-agents" target="_blank" rel="noopener noreferrer" class="github-link">
                    <img src="../assets/shared/github-mark.svg" alt="GitHub" width="24" height="24">
                </a>
                <a href="index.html">Multi-Agent Orchestration with Azure AI Travel Agents</a>
            </div>
            <div class="nav-middle">
            </div>
            <div class="nav-right">
                <a href="../index.html" class="nav-link">Change Theme</a>
                <a href="#" class="nav-link quest-map-trigger">Quests</a>
                <button class="theme-toggle-btn" aria-label="Switch to light mode" type="button">
                    <div class="theme-toggle-slider">
                        <svg class="toggle-icon sun-icon" viewBox="0 0 24 24">
                            <circle cx="12" cy="12" r="4" fill="currentColor"/>
                            <path d="M12 2v2M12 20v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M2 12h2M20 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                        </svg>
                        <svg class="toggle-icon moon-icon" viewBox="0 0 24 24">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                        </svg>
                    </div>
                </button>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="quest-content">
    <h1>Quest 2: Flexible LLM Provider Strategy</h1>
<hr>
<p>The Azure AI Travel Agents project showcases a modular and dynamic approach to integrating multiple large language model (LLM) providers. At the heart of this flexibility is a provider strategy that allows developers to switch between Azure OpenAI, GitHub-hosted models, Ollama models, and more by simply updating environment variables. This quest delves into how the provider factory pattern is implemented, ensuring seamless configuration and runtime adaptability. Explore how this architecture leverages environment detection, token-based authentication, and modular design to support diverse enterprise needs.</p>
<h2>Key Takeaways</h2>
<p>After completing this quest, you will understand:</p>
<ul>
<li>üéØ <strong>Provider Factory Pattern</strong>: How the <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function dynamically selects and configures LLM providers based on environment variables.</li>
<li>üîç <strong>Azure Authentication</strong>: The use of <code class="inline-code">DefaultAzureCredential</code> and <code class="inline-code">ManagedIdentityCredential</code> for secure authentication in cloud environments.</li>
<li>‚ö° <strong>Environment-Specific Logic</strong>: How the system adapts to local and production environments with minimal configuration changes.</li>
<li>üí° <strong>Extensibility</strong>: How new LLM providers can be added without modifying the core logic.</li>
</ul>
<h2>File Exploration</h2>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/index.ts</code></a></h3>
<p>This file serves as the entry point for selecting and initializing the appropriate LLM provider. It uses a factory method pattern to dynamically resolve the provider based on the <code class="inline-code">LLM_PROVIDER</code> environment variable. This design ensures that the system remains extensible and easy to configure for different deployment scenarios.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> dynamically selects the LLM provider based on the <code class="inline-code">LLM_PROVIDER</code> environment variable.</li>
<li><code class="inline-code">LLMProvider</code> type defines valid provider options, ensuring type safety.</li>
<li>Error handling ensures that unsupported providers are flagged during initialization.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">import dotenv from &quot;dotenv&quot;;
dotenv.config();

import { llm as azureOpenAI } from &quot;./azure-openai.js&quot;;
import { llm as foundryLocal } from &quot;./foundry-local.js&quot;;
import { llm as githubModels } from &quot;./github-models.js&quot;;
import { llm as dockerModels } from &quot;./docker-models.js&quot;;
import { llm as ollamaModels } from &quot;./ollama-models.js&quot;;

type LLMProvider =
  | &quot;azure-openai&quot;
  | &quot;github-models&quot;
  | &quot;foundry-local&quot;
  | &quot;docker-models&quot;
  | &quot;ollama-models&quot;;

const provider = (process.env.LLM_PROVIDER || &quot;&quot;) as LLMProvider;

export const llm = async () =&gt; {
  switch (provider) {
    case &quot;azure-openai&quot;:
      return azureOpenAI();
    case &quot;github-models&quot;:
      return githubModels();
    case &quot;docker-models&quot;:
      return dockerModels();
    case &quot;ollama-models&quot;:
      return ollamaModels();
    case &quot;foundry-local&quot;:
      return foundryLocal();
    default:
      throw new Error(
        `Unknown LLM_PROVIDER &quot;${provider}&quot;. Valid options are: azure-openai, github-models, foundry-local, docker-models, ollama-models.`
      );
  }
};
</code></pre>
<ul>
<li>The <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function uses a switch statement for clear and maintainable provider selection logic.</li>
<li>The <code class="inline-code">LLMProvider</code> type ensures that only predefined providers can be used, reducing runtime errors.</li>
<li>The environment variable-driven design allows quick changes to the LLM provider without modifying the codebase.</li>
<li>Error handling provides informative feedback when an unsupported provider is configured.</li>
<li>This modular approach simplifies the addition of new providers by isolating their configuration logic.</li>
</ul>
<hr>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/azure-openai.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/azure-openai.ts</code></a></h3>
<p>This file defines the Azure OpenAI provider, leveraging Azure&#39;s authentication mechanisms to securely connect to the service. It includes support for both local and production environments, demonstrating robust environment-specific logic.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> configures Azure OpenAI with environment detection for local and production setups.</li>
<li><code class="inline-code">DefaultAzureCredential</code> and <code class="inline-code">ManagedIdentityCredential</code> provide secure authentication options.</li>
<li><code class="inline-code">getBearerTokenProvider()</code> generates tokens for Azure Cognitive Services.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">import { openai } from &quot;llamaindex&quot;;
import {
  DefaultAzureCredential,
  getBearerTokenProvider,
  ManagedIdentityCredential,
} from &quot;@azure/identity&quot;;

const AZURE_COGNITIVE_SERVICES_SCOPE =
  &quot;https://cognitiveservices.azure.com/.default&quot;;

export const llm = async () =&gt; {
  console.log(&quot;Using Azure OpenAI&quot;);

  const isRunningInLocalDocker = process.env.IS_LOCAL_DOCKER_ENV === &quot;true&quot;;
  
  if (isRunningInLocalDocker) {
    console.log(
      &quot;Running in local Docker environment, Azure Managed Identity is not supported. Authenticating with apiKey.&quot;
    );
    
    return openai({
      azure: {
        endpoint: process.env.AZURE_OPENAI_ENDPOINT,
        deployment: process.env.AZURE_OPENAI_DEPLOYMENT,
        apiKey: process.env.AZURE_OPENAI_API_KEY,
      },
    });
  }
  
  let credential: any = new DefaultAzureCredential();
  const clientId = process.env.AZURE_CLIENT_ID;
  if (clientId) {
    console.log(&quot;Using Azure Client ID:&quot;, clientId);
    credential = new ManagedIdentityCredential({
      clientId,
    });
  }

  const azureADTokenProvider = getBearerTokenProvider(
    credential,
    AZURE_COGNITIVE_SERVICES_SCOPE
  );

  return openai({
    azure: {
      azureADTokenProvider,
      endpoint: process.env.AZURE_OPENAI_ENDPOINT,
      deployment: process.env.AZURE_OPENAI_DEPLOYMENT,
    },
  });
};
</code></pre>
<ul>
<li>The <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function adapts to both local and production environments using the <code class="inline-code">IS_LOCAL_DOCKER_ENV</code> variable.</li>
<li><code class="inline-code">DefaultAzureCredential</code> provides a unified authentication method for production environments.</li>
<li><code class="inline-code">ManagedIdentityCredential</code> supports specific client IDs, enhancing flexibility in multi-tenant scenarios.</li>
<li>The <code class="inline-code">getBearerTokenProvider()</code> function generates tokens dynamically, ensuring secure and up-to-date authentication.</li>
<li>This setup demonstrates best practices for integrating cloud services with secure authentication.</li>
</ul>
<hr>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/github-models.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/github-models.ts</code></a></h3>
<p>This file implements the GitHub Models provider, showcasing a straightforward integration with a custom API endpoint. It highlights the simplicity of adding new providers to the system.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> configures the GitHub Models provider with a custom endpoint.</li>
<li>API key authentication ensures secure access to the service.</li>
<li>Environment variables define the endpoint and model, simplifying configuration.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">import { openai } from &quot;llamaindex&quot;;

export const llm = async () =&gt; {
  console.log(&quot;Using GitHub Models&quot;);
  return openai({
    baseURL: &quot;https://models.inference.ai.azure.com&quot;,
    apiKey: process.env.GITHUB_TOKEN,
    model: process.env.GITHUB_MODEL,
  });
};
</code></pre>
<ul>
<li>The <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function uses <code class="inline-code">baseURL</code> to define a custom endpoint for the GitHub Models API.</li>
<li>API key authentication is managed via the <code class="inline-code">GITHUB_TOKEN</code> environment variable.</li>
<li>The <code class="inline-code">GITHUB_MODEL</code> variable allows dynamic model selection without code changes.</li>
<li>This provider demonstrates how to integrate third-party services with minimal overhead.</li>
<li>The clear and concise implementation ensures maintainability and ease of extension.</li>
</ul>
<hr>
<h2>Helpful Hints</h2>
<ul>
<li>Study how the <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function in <code class="inline-code">index.ts</code> uses the factory pattern to select providers dynamically.</li>
<li>Observe the use of environment variables for configuration, which simplifies deployment across different environments.</li>
<li>Explore the authentication mechanisms in <code class="inline-code">azure-openai.ts</code> to understand secure integration with Azure services.</li>
</ul>
<h2>Try This</h2>
<p>Challenge yourself to deepen your understanding:</p>
<ol>
<li><p><strong>Add a New Provider</strong>: Implement a new LLM provider for a hypothetical service. Create a new file, define the <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function, and register it in <code class="inline-code">index.ts</code>. Test its integration by setting the <code class="inline-code">LLM_PROVIDER</code> variable.</p>
</li>
<li><p><strong>Trace Authentication Flow</strong>: Add debug logs in <code class="inline-code">azure-openai.ts</code> to trace the token generation process. Observe how <code class="inline-code">DefaultAzureCredential</code> and <code class="inline-code">ManagedIdentityCredential</code> are used.</p>
</li>
<li><p><strong>Custom Error Messages</strong>: Enhance the error handling in <code class="inline-code">index.ts</code> to suggest valid provider options when an unsupported provider is specified. This will improve developer experience.</p>
</li>
</ol>
<hr>
<p>Excellent work! Continue to the next quest to uncover more mysteries of this multi-agent ecosystem.</p>
<p>üéâ Achievement unlocked: Your &quot;Flexible LLM Provider Strategy&quot; implementation has successfully passed its first milestone test case‚Äîkeep refactoring towards the 5/5 deployment goal! üöÄ‚ö°</p>

</div>


      <div class="quest-navigation quest-navigation-bottom">
        <a href="quest-1.html" class="prev-quest-btn">‚Üê Previous: Quest 1</a>
        <a href="quest-3.html" class="next-quest-btn">Next: Quest 3 ‚Üí</a>
      </div>
    
    </div>
    
    <footer class="footer">
        <div class="footer-content">
            <span>Created using <a href="https://github.com/DanWahlin/ai-repo-adventures" target="_blank" rel="noopener noreferrer" class="repo-link">AI Repo Adventures</a></span>
        </div>
    </footer>
    
    <!-- Quest Navigator Script (for navbar Quests button functionality) -->
    <script src="../assets/shared/quest-navigator.js"></script>
    <!-- Theme Toggle Script (for light/dark mode toggle) -->
    <script src="../assets/theme-toggle.js"></script>
</body>
</html>