<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM Provider Strategy - Repo Adventure</title>
    <link rel="stylesheet" href="assets/theme.css">
    <link rel="stylesheet" href="../assets/shared/quest-navigator.css">
    <link rel="stylesheet" href="../assets/theme-toggle.css">
    <!-- Prism.js for syntax highlighting -->
    <!-- Using minimal theme since we override all styles with our custom theme -->
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <script>
        // Configure Prism autoloader for syntax highlighting
        if (window.Prism && window.Prism.plugins && window.Prism.plugins.autoloader) {
            window.Prism.plugins.autoloader.languages_path = 'https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/';
        }

        // Trigger Prism highlighting after page load
        document.addEventListener('DOMContentLoaded', function() {
            if (window.Prism) {
                window.Prism.highlightAll();
            }
        });
    </script>
</head>
<body class="theme-developer">

    <nav class="navbar">
        <div class="nav-content">
            <div class="nav-left">
                <a href="https://github.com/Azure-Samples/azure-ai-travel-agents" target="_blank" rel="noopener noreferrer" class="github-link">
                    <img src="../assets/shared/github-mark.svg" alt="GitHub" width="24" height="24">
                </a>
                <a href="index.html">Multi-Agent Orchestration with Azure AI Travel Agents</a>
            </div>
            <div class="nav-middle">
            </div>
            <div class="nav-right">
                <a href="../index.html" class="nav-link">Change Adventure</a>
                <a href="#" class="nav-link quest-map-trigger">Quests</a>
                <button class="theme-toggle-btn" aria-label="Switch to light mode" type="button">
                    <div class="theme-toggle-slider">
                        <svg class="toggle-icon sun-icon" viewBox="0 0 24 24">
                            <circle cx="12" cy="12" r="4" fill="currentColor"/>
                            <path d="M12 2v2M12 20v2M4.22 4.22l1.42 1.42M18.36 18.36l1.42 1.42M2 12h2M20 12h2M4.22 19.78l1.42-1.42M18.36 5.64l1.42-1.42" stroke="currentColor" stroke-width="2" stroke-linecap="round"/>
                        </svg>
                        <svg class="toggle-icon moon-icon" viewBox="0 0 24 24">
                            <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"/>
                        </svg>
                    </div>
                </button>
            </div>
        </div>
    </nav>
    
    <div class="container">
        <div class="quest-content">
    <h1>Quest 2: Flexible LLM Provider Strategy</h1>
<hr>
<p>In the Azure AI Travel Agents system, the LLM Provider Strategy allows seamless integration of diverse language models to power intelligent agent workflows. This modular approach enables dynamic selection of providers, whether local or cloud-based, ensuring adaptability to different environments and requirements. By leveraging provider-specific configurations and authentication mechanisms, developers can orchestrate AI services tailored to their operational context.</p>
<h2>Key Takeaways</h2>
<p>After completing this quest, you will understand:</p>
<ul>
<li>üéØ <strong>Provider Abstraction</strong>: How the <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> factory function dynamically selects LLM providers based on environment variables.</li>
<li>üîç <strong>Azure Authentication</strong>: How <code class="inline-code">DefaultAzureCredential</code> and <code class="inline-code">ManagedIdentityCredential</code> enable secure authentication for Azure OpenAI services.</li>
<li>‚ö° <strong>Error Handling</strong>: How robust error handling ensures valid provider selection and prevents misconfiguration issues.</li>
<li>üí° <strong>Environment-Specific Logic</strong>: How provider configurations adapt to local and production environments seamlessly.</li>
</ul>
<h2>File Exploration</h2>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/index.ts</code></a></h3>
<p>This file serves as the central hub for managing LLM providers. It defines the <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> factory function, which dynamically selects the appropriate language model provider based on the <code class="inline-code">LLM_PROVIDER</code> environment variable. The modular design ensures scalability by supporting multiple providers such as Azure OpenAI, GitHub Models, and Ollama Models.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> dynamically selects the appropriate provider based on the environment variable.</li>
<li><code class="inline-code">LLMProvider</code> type enumerates valid provider options for type safety.</li>
<li>Error handling ensures misconfiguration issues are caught early with descriptive feedback.</li>
<li>Provider-specific modules are imported and integrated seamlessly.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">import dotenv from &quot;dotenv&quot;;
dotenv.config();

import { llm as azureOpenAI } from &quot;./azure-openai.js&quot;;
import { llm as foundryLocal } from &quot;./foundry-local.js&quot;;
import { llm as githubModels } from &quot;./github-models.js&quot;;
import { llm as dockerModels } from &quot;./docker-models.js&quot;;
import { llm as ollamaModels } from &quot;./ollama-models.js&quot;;

type LLMProvider =
  | &quot;azure-openai&quot;
  | &quot;github-models&quot;
  | &quot;foundry-local&quot;
  | &quot;docker-models&quot;
  | &quot;ollama-models&quot;;

const provider = (process.env.LLM_PROVIDER || &quot;&quot;) as LLMProvider;

export const llm = async () =&gt; {
  switch (provider) {
    case &quot;azure-openai&quot;:
      return azureOpenAI();
    case &quot;github-models&quot;:
      return githubModels();
    case &quot;docker-models&quot;:
      return dockerModels();
    case &quot;ollama-models&quot;:
      return ollamaModels();
    case &quot;foundry-local&quot;:
      return foundryLocal();
    default:
      throw new Error(
        `Unknown LLM_PROVIDER &quot;${provider}&quot;. Valid options are: azure-openai, github-models, foundry-local, docker-models, ollama-models.`
      );
  }
};
</code></pre>
<ul>
<li>The <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function acts as a factory, dynamically selecting the provider based on the <code class="inline-code">LLM_PROVIDER</code> environment variable.</li>
<li>Type safety is enforced through the <code class="inline-code">LLMProvider</code> type, reducing runtime errors.</li>
<li>Modular imports ensure scalability and maintainability when adding new providers.</li>
<li>Error handling prevents invalid configurations, guiding developers to correct setup issues.</li>
<li>This approach allows seamless switching between providers for diverse operational contexts.</li>
</ul>
<hr>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/azure-openai.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/azure-openai.ts</code></a></h3>
<p>This file implements the configuration for Azure OpenAI services. It supports both local and production environments, using <code class="inline-code">DefaultAzureCredential</code> and <code class="inline-code">ManagedIdentityCredential</code> for secure authentication. The modular design ensures compatibility with Azure&#39;s cognitive services.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> supports local Docker environments and production Azure setups.</li>
<li><code class="inline-code">DefaultAzureCredential</code> provides default authentication for Azure services.</li>
<li><code class="inline-code">ManagedIdentityCredential</code> enables authentication using a specific client ID.</li>
<li><code class="inline-code">getBearerTokenProvider()</code> generates tokens for Azure Cognitive Services.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">import { openai } from &quot;llamaindex&quot;;
import {
  DefaultAzureCredential,
  getBearerTokenProvider,
  ManagedIdentityCredential,
} from &quot;@azure/identity&quot;;

const AZURE_COGNITIVE_SERVICES_SCOPE =
  &quot;https://cognitiveservices.azure.com/.default&quot;;

export const llm = async () =&gt; {
  console.log(&quot;Using Azure OpenAI&quot;);

  const isRunningInLocalDocker = process.env.IS_LOCAL_DOCKER_ENV === &quot;true&quot;;
  
  if (isRunningInLocalDocker) {
    console.log(
      &quot;Running in local Docker environment, Azure Managed Identity is not supported. Authenticating with apiKey.&quot;
    );
    
    return openai({
      azure: {
        endpoint: process.env.AZURE_OPENAI_ENDPOINT,
        deployment: process.env.AZURE_OPENAI_DEPLOYMENT,
        apiKey: process.env.AZURE_OPENAI_API_KEY,
      },
    });
  }
  
  let credential: any = new DefaultAzureCredential();
  const clientId = process.env.AZURE_CLIENT_ID;
  if (clientId) {
    console.log(&quot;Using Azure Client ID:&quot;, clientId);
    credential = new ManagedIdentityCredential({
      clientId,
    });
  }

  const azureADTokenProvider = getBearerTokenProvider(
    credential,
    AZURE_COGNITIVE_SERVICES_SCOPE
  );

  return openai({
    azure: {
      azureADTokenProvider,
      endpoint: process.env.AZURE_OPENAI_ENDPOINT,
      deployment: process.env.AZURE_OPENAI_DEPLOYMENT,
    },
  });
};
</code></pre>
<ul>
<li>The <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function adapts to local and production environments, ensuring flexibility in deployment.</li>
<li><code class="inline-code">DefaultAzureCredential</code> and <code class="inline-code">ManagedIdentityCredential</code> provide secure authentication options for Azure services.</li>
<li>The <code class="inline-code">getBearerTokenProvider()</code> method generates tokens required to access Azure Cognitive Services endpoints.</li>
<li>Environment-specific logic ensures compatibility with both local Docker setups and Azure production environments.</li>
<li>This design demonstrates best practices for secure and scalable cloud service integration.</li>
</ul>
<hr>
<h3><span class="header-prefix">File:</span> <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/github-models.ts" target="_blank" rel="noopener noreferrer"><code class="inline-code">src/api/src/orchestrator/llamaindex/providers/github-models.ts</code></a></h3>
<p>This file configures GitHub Models integration, enabling access to inference services hosted on GitHub&#39;s AI platform. It uses API keys for authentication and supports model selection through environment variables.</p>
<h4>Highlights</h4>
<ul>
<li><a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> initializes GitHub Models with API key authentication.</li>
<li>Environment variables configure the base URL, API key, and model name.</li>
<li>Modular design ensures compatibility with other providers.</li>
</ul>
<h4>Code</h4>
<pre><code class="language-typescript">import { openai } from &quot;llamaindex&quot;;

export const llm = async () =&gt; {
  console.log(&quot;Using GitHub Models&quot;);
  return openai({
    baseURL: &quot;https://models.inference.ai.azure.com&quot;,
    apiKey: process.env.GITHUB_TOKEN,
    model: process.env.GITHUB_MODEL,
  });
};
</code></pre>
<ul>
<li>The <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function initializes GitHub Models with API key authentication for secure access.</li>
<li>Environment variables define the base URL, API key, and model name, ensuring flexibility in configuration.</li>
<li>Modular integration allows seamless switching between providers without modifying core logic.</li>
<li>This approach highlights the importance of environment-driven configurations for multi-provider systems.</li>
<li>The design ensures compatibility with GitHub&#39;s AI inference platform, expanding the system&#39;s capabilities.</li>
</ul>
<hr>
<h2>Helpful Hints</h2>
<ul>
<li>Study how <code class="inline-code">DefaultAzureCredential</code> and <code class="inline-code">ManagedIdentityCredential</code> simplify authentication for Azure services.</li>
<li>Observe how environment variables drive provider selection, enabling flexible deployments.</li>
<li>Explore error handling patterns to understand how invalid configurations are prevented.</li>
</ul>
<h2>Try This</h2>
<p>Challenge yourself to deepen your understanding:</p>
<ol>
<li><p><strong>Add a New Provider</strong>: Implement a new LLM provider configuration in <code class="inline-code">src/api/src/orchestrator/llamaindex/providers/</code>. Use an API key-based authentication and register it in <code class="inline-code">index.ts</code>.</p>
<ul>
<li>Example: Create a provider for Hugging Face models with environment variables for API keys and endpoints.</li>
</ul>
</li>
<li><p><strong>Trace Provider Initialization</strong>: Add logging statements to each <a href="https://github.com/Azure-Samples/azure-ai-travel-agents/blob/main/src/api/src/orchestrator/llamaindex/providers/index.ts#L19" target="_blank" rel="noopener noreferrer"><code class="inline-code">llm()</code></a> function to trace the initialization process. Run the system in both local and production environments to observe how providers are selected dynamically.</p>
</li>
<li><p><strong>Enhance Error Handling</strong>: Modify the error handling in <code class="inline-code">index.ts</code> to suggest valid provider options when an unknown provider is specified. This improves developer experience and reduces debugging time.</p>
</li>
</ol>
<hr>
<p>Excellent work! Continue to the next quest to uncover more mysteries of the multi-agent orchestration system.</p>
<p>üéâ Quest &quot;LLM Provider Strategy&quot; successfully deployed‚Äîyour progress bar just ticked to 20%, keep refactoring your skills and iterating toward mastery! üöÄüíé‚ö°</p>

</div>


      <div class="quest-navigation quest-navigation-bottom">
        <a href="quest-1.html" class="prev-quest-btn">‚Üê Previous: Quest 1</a>
        <a href="quest-3.html" class="next-quest-btn">Next: Quest 3 ‚Üí</a>
      </div>
    
    </div>
    
    <footer class="footer">
        <div class="footer-content">
            <span>Created using <a href="https://github.com/DanWahlin/ai-repo-adventures" target="_blank" rel="noopener noreferrer" class="repo-link">AI Repo Adventures</a></span>
        </div>
    </footer>
    
    <!-- Quest Navigator Script (for navbar Quests button functionality) -->
    <script src="../assets/shared/quest-navigator.js"></script>
    <!-- Theme Toggle Script (for light/dark mode toggle) -->
    <script src="../assets/theme-toggle.js"></script>
</body>
</html>